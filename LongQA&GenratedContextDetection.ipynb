{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8504444,"sourceType":"datasetVersion","datasetId":5075882}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Persepolise Group\n\n\n\n## Members:\n\n   ### *Mohsen SHAHVERDIKONDORI*\n   ### *Ali NOSHAD*\n   ### *Mohammad Hosein BEHZADIFARD*\n   ### *Sajjad SALARI*","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\nfrom transformers import BertTokenizer, BertForQuestionAnswering, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AdamW\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom tqdm import tqdm\nfrom random import choice, randint\nimport functools\nfrom time import time\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-26T18:14:40.809401Z","iopub.execute_input":"2024-05-26T18:14:40.810075Z","iopub.status.idle":"2024-05-26T18:14:58.872194Z","shell.execute_reply.started":"2024-05-26T18:14:40.810041Z","shell.execute_reply":"2024-05-26T18:14:58.871366Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-26 18:14:50.611646: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-26 18:14:50.611750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-26 18:14:50.736945: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"eli5_category\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:14:58.874020Z","iopub.execute_input":"2024-05-26T18:14:58.874620Z","iopub.status.idle":"2024-05-26T18:15:38.601683Z","shell.execute_reply.started":"2024-05-26T18:14:58.874593Z","shell.execute_reply":"2024-05-26T18:15:38.600863Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for eli5_category contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eli5_category\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"215473abb244417182c1c6fc3445bc27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/12.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8135046efcb497083b8eb1a6ea90a12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/62.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a609b6415e644bc98bf3443735c302e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.00M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157a517899524ec9ad6b313d9c26c7e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.76M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"566ca8705a7b4d358099e5412a4a2820"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"490bbb3e75934186a8de497f9acc18e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/91772 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"286381ac55784866bbd9d09443c5679b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation1 split:   0%|          | 0/5446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"897c4dde9f5a4aa8baa06362f681882e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation2 split:   0%|          | 0/2375 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f43c1941cd534514aa9b07f370844822"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1b8ec72c7c64daca701e8b23cf2d3bf"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:38.602929Z","iopub.execute_input":"2024-05-26T18:15:38.603271Z","iopub.status.idle":"2024-05-26T18:15:38.611158Z","shell.execute_reply.started":"2024-05-26T18:15:38.603239Z","shell.execute_reply":"2024-05-26T18:15:38.610015Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n        num_rows: 91772\n    })\n    validation1: Dataset({\n        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n        num_rows: 5446\n    })\n    validation2: Dataset({\n        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n        num_rows: 2375\n    })\n    test: Dataset({\n        features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n        num_rows: 5411\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset[\"train\"][3]['title']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:38.613939Z","iopub.execute_input":"2024-05-26T18:15:38.614234Z","iopub.status.idle":"2024-05-26T18:15:39.330496Z","shell.execute_reply.started":"2024-05-26T18:15:38.614208Z","shell.execute_reply":"2024-05-26T18:15:39.329597Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'Why is it that we calm down when we take a deep breath, hold it for a few seconds and exhale?'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading Pre-trained models","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizer, LongformerForQuestionAnswering\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\nfrom transformers import AdamW, AutoModel, AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n# Load tokenizer and model\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\nmodel = T5ForConditionalGeneration.from_pretrained('t5-small').to(\"cuda:0\")\nparam_dict = torch.load(\"/kaggle/input/t5-models/results (1)/QAModel_2.pth\")  # has model weights, optimizer, and scheduler states\nmodel.load_state_dict(param_dict[\"model\"])\n\n'''tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large\").to(\"cuda:0\")'''","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:39.332087Z","iopub.execute_input":"2024-05-26T18:15:39.332749Z","iopub.status.idle":"2024-05-26T18:15:51.389712Z","shell.execute_reply.started":"2024-05-26T18:15:39.332698Z","shell.execute_reply":"2024-05-26T18:15:51.388672Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd5904067a684af991d6d40a1769fba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3432c0d7ca30491d9c25515cc063589a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a525868d81400682929427ad64bf51"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bc42f0c470a4718846ab90786201bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da575c63d2024af4875ddfa3d54f4431"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7295fedaa4641ea98a8ca2e5d6f9d1d"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large\").to(\"cuda:0\")'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Create the class for instance retirival and random sampling on answers","metadata":{}},{"cell_type":"code","source":"class ELI5Dataset(Dataset):\n    def __init__(self, examples_array, extra_answer_threshold=3, min_answer_length=64, training=True, n_samples=None):\n        self.data = examples_array\n        self.answer_thres = extra_answer_threshold\n        self.min_length = min_answer_length\n        self.training = training\n        self.n_samples = self.data.num_rows if n_samples is None else n_samples\n\n    def __len__(self):\n        return self.n_samples\n\n    def make_example(self, idx):\n        example = self.data[idx]\n        question = example[\"title\"]\n        if self.training:\n            answers = [a for i, (a, sc) in enumerate(zip(example[\"answers\"][\"text\"], example[\"answers\"][\"score\"]))]\n            answer_tab = choice(answers).split(\" \")\n            start_idx = randint(0, max(0, len(answer_tab) - self.min_length))\n            answer_span = \" \".join(answer_tab[start_idx:])\n        else:\n            answer_span = example[\"answers\"][\"text\"][0]\n        return (question, answer_span)\n\n    def __getitem__(self, idx):\n        return self.make_example(idx % self.data.num_rows)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.390908Z","iopub.execute_input":"2024-05-26T18:15:51.391206Z","iopub.status.idle":"2024-05-26T18:15:51.401562Z","shell.execute_reply.started":"2024-05-26T18:15:51.391180Z","shell.execute_reply":"2024-05-26T18:15:51.400551Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Creat a batch ","metadata":{}},{"cell_type":"code","source":"def create_batch(qa_list, tokenizer, max_len=512, device=\"cuda:0\"):\n    q_ls = [q for q, a in qa_list]\n    a_ls = [a for q, a in qa_list]\n    q_toks = tokenizer(q_ls, max_length=max_len, padding=\"max_length\", truncation=True)\n    q_ids, q_mask = (\n        torch.LongTensor(q_toks[\"input_ids\"]).to(device),\n        torch.LongTensor(q_toks[\"attention_mask\"]).to(device),\n    )\n    a_toks = tokenizer(a_ls, max_length=max_len, padding=\"max_length\", truncation=True)\n    a_ids, a_mask = (\n        torch.LongTensor(a_toks[\"input_ids\"]).to(device),\n        torch.LongTensor(a_toks[\"attention_mask\"]).to(device),\n    )\n    lm_labels = a_ids[:, 1:].contiguous().clone()\n    lm_labels[a_mask[:, 1:].contiguous() == 0] = -100\n    model_inputs = {\n        \"input_ids\": q_ids,\n        \"attention_mask\": q_mask,\n        \"decoder_input_ids\": a_ids[:, :-1].contiguous(),\n        \"labels\": lm_labels,\n    }\n    return model_inputs\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.402708Z","iopub.execute_input":"2024-05-26T18:15:51.403004Z","iopub.status.idle":"2024-05-26T18:15:51.416273Z","shell.execute_reply.started":"2024-05-26T18:15:51.402980Z","shell.execute_reply":"2024-05-26T18:15:51.415322Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"(dataset['train'])","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.417512Z","iopub.execute_input":"2024-05-26T18:15:51.417909Z","iopub.status.idle":"2024-05-26T18:15:51.428194Z","shell.execute_reply.started":"2024-05-26T18:15:51.417884Z","shell.execute_reply":"2024-05-26T18:15:51.427291Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['q_id', 'title', 'selftext', 'category', 'subreddit', 'answers', 'title_urls', 'selftext_urls'],\n    num_rows: 91772\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Split the data into train, validation, test","metadata":{}},{"cell_type":"code","source":"qar_train = ELI5Dataset(dataset['train'], training=True)\nqar_valid = ELI5Dataset(dataset['validation1'], training=False)\nqar_test = ELI5Dataset(dataset['test'], training=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.429520Z","iopub.execute_input":"2024-05-26T18:15:51.429847Z","iopub.status.idle":"2024-05-26T18:15:51.436761Z","shell.execute_reply.started":"2024-05-26T18:15:51.429814Z","shell.execute_reply":"2024-05-26T18:15:51.435855Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_sampler = RandomSampler(qar_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.440967Z","iopub.execute_input":"2024-05-26T18:15:51.441236Z","iopub.status.idle":"2024-05-26T18:15:51.450805Z","shell.execute_reply.started":"2024-05-26T18:15:51.441213Z","shell.execute_reply":"2024-05-26T18:15:51.449953Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\ncollate_fn_test = functools.partial(\n        create_batch, tokenizer=tokenizer, max_len=512, device=\"cuda:0\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.451829Z","iopub.execute_input":"2024-05-26T18:15:51.452106Z","iopub.status.idle":"2024-05-26T18:15:51.461320Z","shell.execute_reply.started":"2024-05-26T18:15:51.452082Z","shell.execute_reply":"2024-05-26T18:15:51.460474Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(\"Size of Training Set: \", len(qar_train))\nprint(\"Size of Validation Set: \", len(qar_valid))\nprint(\"Size of Test Set: \", len(qar_test))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.462432Z","iopub.execute_input":"2024-05-26T18:15:51.462760Z","iopub.status.idle":"2024-05-26T18:15:51.471072Z","shell.execute_reply.started":"2024-05-26T18:15:51.462735Z","shell.execute_reply":"2024-05-26T18:15:51.470055Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Size of Training Set:  91772\nSize of Validation Set:  5446\nSize of Test Set:  5411\n","output_type":"stream"}]},{"cell_type":"code","source":"dataloader_test = DataLoader(qar_test, batch_size=4, sampler=test_sampler, collate_fn=collate_fn_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.472350Z","iopub.execute_input":"2024-05-26T18:15:51.472753Z","iopub.status.idle":"2024-05-26T18:15:51.480231Z","shell.execute_reply.started":"2024-05-26T18:15:51.472694Z","shell.execute_reply":"2024-05-26T18:15:51.479299Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"epoch_iterator_test = tqdm(dataloader_test, desc=\"Iteration\", disable=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.481372Z","iopub.execute_input":"2024-05-26T18:15:51.482160Z","iopub.status.idle":"2024-05-26T18:15:51.489897Z","shell.execute_reply.started":"2024-05-26T18:15:51.482134Z","shell.execute_reply":"2024-05-26T18:15:51.489024Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Creating an function for doing a epoch of the training","metadata":{}},{"cell_type":"code","source":"def one_epoch_training(model, dataset, tokenizer, optimizer, scheduler, epoch):\n    train_sampler = RandomSampler(dataset)\n    collate_fn_train = functools.partial(\n        create_batch, tokenizer=tokenizer, max_len=512, device=\"cuda:0\"\n    )\n    dataloader_train = DataLoader(dataset, batch_size=4, sampler=train_sampler, collate_fn=collate_fn_train)\n    epoch_iterator_train = tqdm(dataloader_train, desc=\"Iteration\", disable=True)\n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    for step, batch_inputs in enumerate(epoch_iterator_train):\n        if batch_inputs != 1:\n            pre_loss = model(**batch_inputs)\n            loss = pre_loss.loss\n            loss.backward()\n            # optimizer\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n            # some printing within the epoch\n            loc_loss += loss.item()\n            loc_steps += 1\n            if step % 100 == 0 or step == 1:\n                print(\n                    \"{:2d} {:5d} of {:5d} \\t L: {:.10f} \\t -- {:.3f}\".format(\n                        epoch,\n                        step,\n                        len(dataset) // 8,\n                        loc_loss / loc_steps,\n                        time() - st_time,\n                    )\n                )\n                loc_loss = 0\n                loc_steps = 0","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.491219Z","iopub.execute_input":"2024-05-26T18:15:51.491782Z","iopub.status.idle":"2024-05-26T18:15:51.501551Z","shell.execute_reply.started":"2024-05-26T18:15:51.491741Z","shell.execute_reply":"2024-05-26T18:15:51.500499Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Creating a function for doing a epoch of validation","metadata":{}},{"cell_type":"code","source":"def one_epoch_evaluation(model, dataset, tokenizer, optimizer, scheduler, epoch):\n    valid_sampler = RandomSampler(dataset)\n    collate_fn_valid = functools.partial(\n        create_batch, tokenizer=tokenizer, max_len=512, device=\"cuda:0\"\n    )\n    dataloader_valid = DataLoader(dataset, batch_size=4, sampler=valid_sampler, collate_fn=collate_fn_valid)\n    epoch_iterator_valid = tqdm(dataloader_valid, desc=\"Iteration\", disable=True)\n    # accumulate loss since last print\n    loc_steps = 0\n    loc_loss = 0.0\n    st_time = time()\n    with torch.no_grad():\n        for step, batch_inputs in enumerate(epoch_iterator_valid):\n            if batch_inputs != 1:\n                pre_loss = model(**batch_inputs)\n                loss = pre_loss.loss\n                loc_loss += loss.item()\n                loc_steps += 1\n                if step % 100 == 0:\n                    print(\n                        \"{:5d} of {:5d} \\t L: {:.10f} \\t -- {:.3f}\".format(\n                            step,\n                            len(dataset) // 8,\n                            loc_loss / loc_steps,\n                            time() - st_time,\n                        )\n                    )\n    \n    print(\n        \"Total \\t L: {:.3f} \\t -- {:.3f}\".format(\n            loc_loss / loc_steps,\n            time() - st_time,\n        )\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.502722Z","iopub.execute_input":"2024-05-26T18:15:51.503081Z","iopub.status.idle":"2024-05-26T18:15:51.515215Z","shell.execute_reply.started":"2024-05-26T18:15:51.503051Z","shell.execute_reply":"2024-05-26T18:15:51.514316Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Function for training and validation","metadata":{}},{"cell_type":"code","source":"def train_question_answering(model, tokenizer, train_dataset, valid_dataset):\n    optimizer = AdamW(model.parameters(), lr=2e-4, eps=1e-8)\n    scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=400,\n            num_training_steps=(3 + 1) * math.ceil(len(train_dataset) /8),)\n    for epoch in range(2):\n        one_epoch_training(model, train_dataset, tokenizer, optimizer, scheduler,epoch)\n        m_save_dict = {\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"scheduler\": scheduler.state_dict(),\n        }\n        one_epoch_evaluation(model, valid_dataset, tokenizer, optimizer, scheduler, epoch)\n        torch.save(m_save_dict, \"{}_{}.pth\".format(\"QAModel\", epoch))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.516335Z","iopub.execute_input":"2024-05-26T18:15:51.516690Z","iopub.status.idle":"2024-05-26T18:15:51.528098Z","shell.execute_reply.started":"2024-05-26T18:15:51.516657Z","shell.execute_reply":"2024-05-26T18:15:51.527206Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#train_question_answering(model, tokenizer, qar_train, qar_valid)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.529228Z","iopub.execute_input":"2024-05-26T18:15:51.529521Z","iopub.status.idle":"2024-05-26T18:15:51.541005Z","shell.execute_reply.started":"2024-05-26T18:15:51.529498Z","shell.execute_reply":"2024-05-26T18:15:51.540049Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Testing on the some of the data","metadata":{}},{"cell_type":"code","source":"# Function to generate answers for a given question\ndef generate_answer(question, model, tokenizer, device=\"cuda:0\", max_len=512):\n    # Prefix the question with 'question:' and tokenize it\n    input_text = f\"question: {question} </s>\"\n    input_ids = tokenizer.encode(question, return_tensors='pt').to(device)\n\n    # Generate answer IDs\n    output_ids = model.generate(input_ids, max_length=max_len, num_beams=4, early_stopping=True)\n\n    # Decode the generated answer IDs\n    generated_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\n    return generated_answer","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.542166Z","iopub.execute_input":"2024-05-26T18:15:51.542749Z","iopub.status.idle":"2024-05-26T18:15:51.550186Z","shell.execute_reply.started":"2024-05-26T18:15:51.542723Z","shell.execute_reply":"2024-05-26T18:15:51.549237Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"question = dataset[\"train\"][6]['title']","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.551470Z","iopub.execute_input":"2024-05-26T18:15:51.552206Z","iopub.status.idle":"2024-05-26T18:15:51.559388Z","shell.execute_reply.started":"2024-05-26T18:15:51.552173Z","shell.execute_reply":"2024-05-26T18:15:51.558593Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"dataset[\"train\"][6]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.560484Z","iopub.execute_input":"2024-05-26T18:15:51.560901Z","iopub.status.idle":"2024-05-26T18:15:51.571037Z","shell.execute_reply.started":"2024-05-26T18:15:51.560875Z","shell.execute_reply":"2024-05-26T18:15:51.570143Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'q_id': '5ld008',\n 'title': 'Why does lights make a \"blinking\" effect when looking at them from far away?',\n 'selftext': '',\n 'category': 'Other',\n 'subreddit': 'explainlikeimfive',\n 'answers': {'a_id': ['dbusvko'],\n  'text': [\"Because of the air in between you and the light source. The air is at slightly different temperatures so it's like a heat shimmer effect you get above a fire, also the air is full of dust and other stuff we don't normally notice, but at distances it blocks a bit of the light.\"],\n  'score': [5],\n  'text_urls': [[]]},\n 'title_urls': ['url'],\n 'selftext_urls': ['url']}"},"metadata":{}}]},{"cell_type":"code","source":"generate_answer(question, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:51.572296Z","iopub.execute_input":"2024-05-26T18:15:51.572775Z","iopub.status.idle":"2024-05-26T18:15:53.567278Z","shell.execute_reply.started":"2024-05-26T18:15:51.572708Z","shell.execute_reply":"2024-05-26T18:15:53.566301Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"', but it\\'s not a \"blinking\" effect, it\\'s a combination of light and light. When you look at something from far away, you\\'re able to see the light from far away, but it\\'s not a \"blinking effect\" either. It\\'s not a \"blinking effect\", it\\'s a \"blinking effect\".'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Generated Context Detection","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:35:22.326479Z","iopub.execute_input":"2024-05-26T18:35:22.326866Z","iopub.status.idle":"2024-05-26T18:35:22.635633Z","shell.execute_reply.started":"2024-05-26T18:35:22.326837Z","shell.execute_reply":"2024-05-26T18:35:22.634582Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"true_ans = []\npred_ans =[]\nfor i in range(1000):\n    true_ans.append(dataset[\"train\"][i]['answers']['text'][0])\n    pred_ans.append(generate_answer(dataset[\"train\"][i]['title'], model, tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:15:53.568564Z","iopub.execute_input":"2024-05-26T18:15:53.568931Z","iopub.status.idle":"2024-05-26T18:31:03.071926Z","shell.execute_reply.started":"2024-05-26T18:15:53.568901Z","shell.execute_reply":"2024-05-26T18:31:03.070947Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Function to generate text embeddings\ndef get_text_embedding(text):\n    # Tokenize the input text\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    \n    # Get the hidden states from the model\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    # Mean pooling to get the sentence embedding\n    embeddings = outputs.last_hidden_state.mean(dim=1)\n    \n    return embeddings.squeeze().numpy()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:37:26.941973Z","iopub.execute_input":"2024-05-26T18:37:26.942854Z","iopub.status.idle":"2024-05-26T18:37:26.948065Z","shell.execute_reply.started":"2024-05-26T18:37:26.942820Z","shell.execute_reply":"2024-05-26T18:37:26.947054Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Combine lists into a DataFrame with labels\ndata = {\n    \"text\": true_ans + pred_ans,\n    \"label\": [0] * len(true_ans) + [1] * len(pred_ans)  # 0 for human-generated, 1 for AI-generated\n}\n\ndf = pd.DataFrame(data)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:37:27.147394Z","iopub.execute_input":"2024-05-26T18:37:27.148289Z","iopub.status.idle":"2024-05-26T18:37:27.154550Z","shell.execute_reply.started":"2024-05-26T18:37:27.148255Z","shell.execute_reply":"2024-05-26T18:37:27.153604Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Apply the embedding function to the dataset\ndf['embedding'] = df['text'].apply(get_text_embedding)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:37:27.883325Z","iopub.execute_input":"2024-05-26T18:37:27.884137Z","iopub.status.idle":"2024-05-26T18:38:27.653784Z","shell.execute_reply.started":"2024-05-26T18:37:27.884103Z","shell.execute_reply":"2024-05-26T18:38:27.652682Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Split the data\nX = np.stack(df['embedding'].values)\ny = df['label'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:38:27.655721Z","iopub.execute_input":"2024-05-26T18:38:27.656060Z","iopub.status.idle":"2024-05-26T18:38:27.667954Z","shell.execute_reply.started":"2024-05-26T18:38:27.656029Z","shell.execute_reply":"2024-05-26T18:38:27.666938Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print(\"X train shape:\", X_train.shape)\nprint(\"Y trian shape:\", y_train.shape)\nprint(\"X test shape:\", X_test.shape)\nprint(\"Y test shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:38:44.399115Z","iopub.execute_input":"2024-05-26T18:38:44.399473Z","iopub.status.idle":"2024-05-26T18:38:44.405648Z","shell.execute_reply.started":"2024-05-26T18:38:44.399444Z","shell.execute_reply":"2024-05-26T18:38:44.404596Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"X train shape: (1600, 384)\nY trian shape: (1600,)\nX test shape: (400, 384)\nY test shape: (400,)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Classification\nclf = LogisticRegression()\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:38:47.566649Z","iopub.execute_input":"2024-05-26T18:38:47.567454Z","iopub.status.idle":"2024-05-26T18:38:47.631829Z","shell.execute_reply.started":"2024-05-26T18:38:47.567413Z","shell.execute_reply":"2024-05-26T18:38:47.630610Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Predict and Evaluate\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T18:38:49.823354Z","iopub.execute_input":"2024-05-26T18:38:49.823722Z","iopub.status.idle":"2024-05-26T18:38:49.858939Z","shell.execute_reply.started":"2024-05-26T18:38:49.823693Z","shell.execute_reply":"2024-05-26T18:38:49.857589Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.94      0.93      0.93       209\n           1       0.92      0.93      0.93       191\n\n    accuracy                           0.93       400\n   macro avg       0.93      0.93      0.93       400\nweighted avg       0.93      0.93      0.93       400\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}